{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Donors: Response Notebook\n",
    "\n",
    "Este notebook é destinado às respostas do primeiro projeto do *Nanodegree Data Science*. Como todas as perguntas foram feitas em inglês, as respostas também serão feitas em inglês"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Naive Predictor Performace\n",
    "* If we chose a model that always predicted an individual made more than $50,000, what would  that model's accuracy and F-score be on this dataset? You must use the code cell below and assign your results to `'accuracy'` and `'fscore'` to be used later.\n",
    "\n",
    "** Please note ** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place you could start from.\n",
    "\n",
    "** HINT: ** \n",
    "\n",
    "* When we have a model that always predicts '1' (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative('0' value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives/(True Positives + False Positives)) as every prediction that we have made with value '1' that should have '0' becomes a False Positive; therefore our denominator in this case is the total number of records we have in total. \n",
    "* Our Recall score(True Positives/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "The question regards some concepts about metrics in which a model is evaluated: **accuracy** and **F-Score** (which in turn depends on the concepts of **precision** and **recall**).\n",
    "Specifically, those concepts make sense when talking about **binary classification**, i.e. when classifying things in *good/bad*, *healthy/unhealthy*, *viable donor/inviable donor*, etc.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is a more broad term and is not restricted to binary classification nor statistical analysis. It is, in general terms, the closeness of a measurement to the *true value*[¹](#references). So it is a way of describing and measuring observational errors.\n",
    "\n",
    "#### Binary Classification\n",
    "\n",
    "In binary classification, accuracy is a measure, $\\text{accuracy}\\in[0;1]$, that tells how well a model identifies a condition. So it is the ratio of **true results** (results that the model correctly predicted) to **all results** (every prediction made):\n",
    "$$ \\text{accuracy} = \\frac{TP}{N} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'>¹</a>: ISO 5725-1: \"Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions\". ([PT-BR](http://www.abntcatalogo.com.br/norma.aspx?ID=394595), [EN](https://www.iso.org/standard/11833.html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
